{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import absolute\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lofo import LOFOImportance, plot_importance, Dataset\n",
    "import lightgbm as lgbm\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import optuna\n",
    "from IPython.core.display import HTML\n",
    "import dataframe_image as dfi\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import seaborn as sns\n",
    "# pd.options.display.float_format = '{:,.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_in = pd.read_csv('C:/Users/bbloc/OneDrive/Desktop/NYU_Data_Science/Responsible_DS/Project/data/application_train.csv')\n",
    "test_in = pd.read_csv('C:/Users/bbloc/OneDrive/Desktop/NYU_Data_Science/Responsible_DS/Project/data/application_test.csv')\n",
    "print('Train set shape: ', train_in.shape)\n",
    "print('Test set shape: ', test_in.shape)\n",
    "train_in.columns = train_in.columns.str.lower()\n",
    "test_in.columns = test_in.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_in.columns)\n",
    "print([x for x in train_in.columns if x not in test_in.columns])\n",
    "d_types = train_in.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspections.score.describe()\n",
    "# def test_dependency(df, col1, col2):\n",
    "#     '''Tests whether there is total redundancy betwen col1 and col2\n",
    "#        by testing the number of unique combinations'''\n",
    "#     unique_1 = len(df[col1].unique())\n",
    "#     unique_2 = len(df[col2].unique())\n",
    "#     unique_combos = len(df[[col1, col2]].drop_duplicates())\n",
    "#     if unique_1 == 2 and unique_2 == 2:\n",
    "#         if list(sorted(df[col1].unique())) == [0, 1] and unique_combos == max(unique_1, unique_2):\n",
    "#             print(\"Unique combos of {} and {}: {}\".format(col1, col2, unique_combos))\n",
    "\n",
    "# for col1 in train_in.columns:\n",
    "#     for col2 in train_in.columns:\n",
    "#         if col2 == col1 or col1 in ['sk_id_curr', 'target'] or col2 in ['sk_id_curr', 'target']:\n",
    "#             continue\n",
    "#         test_dependency(train_in, col1, col2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_cols = [i for i in train_in.columns if i not in test_in.columns]\n",
    "unique_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if these variables are a unique identifier\n",
    "possible_identifier = ['sk_id_curr']\n",
    "print(\"# of duplicates: \",sum(train_in.duplicated(possible_identifier)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_in.columns:\n",
    "    if len(train_in[col].unique()) > 10:\n",
    "        temp = train_in.copy()\n",
    "        temp['count'] = temp.groupby(col)['sk_id_curr'].transform('count')\n",
    "        temp['percentage'] = temp['count'] / len(temp)\n",
    "        if temp['percentage'].max() > 0.15:\n",
    "            print('{}: {}, {:,}, {:.1%}'.format(col, temp[temp['percentage'] > 0.15].reset_index().loc[0][col], temp[temp['percentage'] > 0.15].reset_index().loc[0]['count'], temp[temp['percentage'] > 0.15].reset_index().loc[0]['percentage']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train_in.columns:\n",
    "    if 'XNA' in train_in[col].unique():\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_profile_metrics(data):\n",
    "    \n",
    "    metrics = pd.DataFrame()\n",
    "    \n",
    "    for col, index in zip(data.columns, range(0, len(data.columns))):\n",
    "        if col == 'target':\n",
    "            continue\n",
    "        \n",
    "        if col == 'days_employed':\n",
    "            data[col] = np.where((data[col] == 365243), np.nan, data[col])\n",
    "        \n",
    "        if isinstance(data.reset_index().loc[0][col], str) or sorted(list(data[col].unique())) == [0, 1]:\n",
    "            data[col] = np.where((data[col].isnull() == True) | (data[col] == 'XNA'), '', data[col])\n",
    "            \n",
    "            missing_perc = len(data[(data[col].isnull() == True) | (data[col] == '')]) / len(data)\n",
    "            \n",
    "            data[col] = np.where((data[col] == 'Yes'), 'Y', data[col])\n",
    "            data[col] = np.where((data[col] == 'No'), 'N', data[col])\n",
    "            \n",
    "            if sorted(list(data[col].unique())) == ['0', '1'] or sorted(list(data[col].unique())) == ['', '0', '1']:\n",
    "                data[col] = data[col].astype(float)\n",
    "            \n",
    "            dtype = data.dtypes[index].name\n",
    "            num_vals = len(list(data[col].unique()))\n",
    "            \n",
    "            if sorted(list(data[col].unique())) == [0, 1] or sorted(list(data[col].unique())) == [0, 1, np.nan] or sorted(list(data[col].unique())) == ['N', 'Y'] or sorted(list(data[col].unique())) == ['', 'N', 'Y']:\n",
    "                perc_yes = len(data[(data[col] == 'Y') | (data[col] == 1)]) / len(data)\n",
    "                metrics = metrics.append({'column': col, 'dtype': dtype, 'perc_yes': perc_yes, 'unique_vals': num_vals, 'missing_perc': missing_perc}, ignore_index=True)\n",
    "            else:\n",
    "                metrics = metrics.append({'column': col, 'dtype': dtype, 'perc_yes': np.nan, 'unique_vals': num_vals, 'missing_perc': missing_perc}, ignore_index=True)\n",
    "        else:\n",
    "            median = data[col].median()\n",
    "            mean = data[col].mean()\n",
    "            min_val = data[col].min()\n",
    "            max_val = data[col].max()\n",
    "            dtype = data.dtypes[index].name\n",
    "            num_vals = len(list(data[col].unique()))\n",
    "            missing_perc = len(data[(data[col].isnull() == True) | (data[col] == '')]) / len(data)\n",
    "            metrics = metrics.append({'column': col, 'dtype': dtype, 'median': median, 'mean': mean, 'min': min_val, 'max': max_val, 'unique_vals': num_vals, 'missing_perc': missing_perc}, ignore_index=True)\n",
    "    \n",
    "    return metrics, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_in.copy()\n",
    "metrics, train = compute_profile_metrics(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads_used = ['EXT_SOURCE_3',\n",
    "             'EXT_SOURCE_2',\n",
    "             'EXT_SOURCE_1',\n",
    "             'NAME_EDUCATION_TYPE',\n",
    "             'CODE_GENDER',\n",
    "             'NAME_INCOME_TYPE',\n",
    "             'ORGANIZATION_TYPE',\n",
    "             'DAYS_EMPLOYED',\n",
    "             'FLOORSMAX_AVG',\n",
    "             'FLOORSMAX_MEDI',\n",
    "             'FLOORSMAX_MODE',\n",
    "             'EMERGENCYSTATE_MODE',\n",
    "             'HOUSETYPE_MODE',\n",
    "             'AMT_GOODS_PRICE',\n",
    "             'REGION_POPULATION_RELATIVE',\n",
    "             'OWN_CAR_AGE',\n",
    "             'DAYS_REGISTRATION',\n",
    "             'OCCUPATION_TYPE',\n",
    "             'FLAG_DOCUMENT_3',\n",
    "             'REG_CITY_NOT_LIVE_CITY',\n",
    "             'FLAG_EMP_PHONE',\n",
    "             'REG_CITY_NOT_WORK_CITY',\n",
    "             'DAYS_ID_PUBLISH',\n",
    "             'DAYS_LAST_PHONE_CHANGE',\n",
    "             'REGION_RATING_CLIENT',\n",
    "             'REGION_RATING_CLIENT_W_CITY',\n",
    "             'DAYS_BIRTH']\n",
    "\n",
    "ads_used_lower = [x.lower() for x in ads_used]\n",
    "pd.options.display.max_rows = 150\n",
    "metrics_all = metrics.copy()\n",
    "metrics_used = metrics.copy()\n",
    "for df, file_name in zip([metrics_all, metrics_used], ['metrics_all_profiled', 'metrics_used_profiled']):\n",
    "    if 'used' in file_name:\n",
    "        df = df[df['column'].isin(ads_used_lower)]\n",
    "    for col in ['missing_perc', 'perc_yes']:\n",
    "        df[col] = df[col].apply('{:.1%}'.format)\n",
    "        df[col] = np.where((df[col] == 'nan%'), '', df[col])\n",
    "    for col in ['median', 'mean', 'min', 'max']:\n",
    "        df[col] = df[col].apply('{:,.1f}'.format)\n",
    "    for col in ['unique_vals']:\n",
    "        df[col] = df[col].astype(int)\n",
    "        df[col] = df[col].apply('{:,}'.format)\n",
    "    if 'all' in file_name:\n",
    "        temp1 = df.copy()\n",
    "        temp2 = df.copy()\n",
    "        temp3 = df.copy()\n",
    "        temp1 = temp1.loc[0:40]\n",
    "        temp2 = temp2.loc[41:80]\n",
    "        temp3 = temp3.loc[81:]\n",
    "        for temp, file_name_temp in zip([temp1, temp2, temp3], ['_1', '_2', '_3']):\n",
    "            dfi.export(temp.style.hide_index().set_table_styles([dict(selector = 'th', props=[('text-align', 'right')])]), 'C:/Users/bbloc/OneDrive/Desktop/NYU_Data_Science/Responsible_DS/Project/figures/{}{}.png'.format(file_name, file_name_temp), max_rows=150)\n",
    "    else:\n",
    "        dfi.export(df.style.hide_index().set_table_styles([dict(selector = 'th', props=[('text-align', 'right')])]), 'C:/Users/bbloc/OneDrive/Desktop/NYU_Data_Science/Responsible_DS/Project/figures/{}.png'.format(file_name), max_rows=150)\n",
    "    if 'used' in file_name:\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dist(df_in, col_list):\n",
    "    \n",
    "    df = df_in.copy()\n",
    "    df = df[col_list]\n",
    "    \n",
    "    for col in df:\n",
    "        plt.figure(figsize=(20,10))\n",
    "        if 'days' in col:\n",
    "            df[col] = df[col] / -365\n",
    "            bins = np.arange(0, df[col].max(), 1)\n",
    "            plt.hist(df[col], bins=bins)\n",
    "            plt.xticks(bins)\n",
    "            title = col.title() + ' In Years'\n",
    "        else:\n",
    "            plt.hist(df[col])\n",
    "            title = col.title()\n",
    "        plt.title('Distribution of {}'.format(title))\n",
    "        plt.xlabel(title.lower())\n",
    "        plt.ylabel('count')\n",
    "        if len(df[col].unique()) > 4:\n",
    "            plt.xticks(rotation=45)\n",
    "        plt.savefig('C:/Users/bbloc/OneDrive/Desktop/NYU_Data_Science/Responsible_DS/Project/figures/{}_hist.png'.format(col))\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "plot_dist(train, ads_used_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = train.copy()\n",
    "    \n",
    "cat_vars = list(metrics[(metrics['dtype'] == 'object') & ((metrics['unique_vals'] > 3) | ((metrics['unique_vals'] == 3) & (metrics['missing_perc'] == 0)))]['column'])\n",
    "\n",
    "for col in temp.columns:\n",
    "    if col in list(metrics[(metrics['dtype'] == 'object')]['column']) and col not in cat_vars:\n",
    "        unique_vals = list(temp[col].unique())\n",
    "        print('{} with {} coded as 0'.format(col, unique_vals[0]))\n",
    "        temp[col] = np.where((temp[col] == unique_vals[0]), 0, 1)\n",
    "        \n",
    "seen = []\n",
    "corrs = pd.DataFrame()\n",
    "for col1 in temp.columns:\n",
    "    if col1 == 'target' or col1[-4:] == 'mode' or col1[-4:] == 'medi' or col1[-4:] == '_avg' or 'flag_document' in col1 or col1 in cat_vars:\n",
    "        continue\n",
    "    for col2 in temp.columns:\n",
    "        if col1 == col2 or col2 + ', ' + col1 in seen or col2 in cat_vars or col2 == 'target':\n",
    "            continue\n",
    "        temp1 = temp.copy()\n",
    "        temp1 = temp1[(temp1[col1].isnull() == False) & (temp1[col1] != '') & (temp1[col2].isnull() == False) & (temp1[col2] != '')]\n",
    "        corr = np.corrcoef(temp1[col1], temp1[col2])[0,1]\n",
    "        seen.append(col1 + ', ' + col2)\n",
    "        corrs = corrs.append({'col1': col1, 'col2': col2, 'corr': corr}, ignore_index=True)\n",
    "\n",
    "corrs_target = pd.DataFrame()\n",
    "for col in temp.columns:\n",
    "    if col not in cat_vars and col != 'target':\n",
    "        temp1 = temp.copy()\n",
    "        temp1 = temp1[(temp1[col].isnull() == False) & (temp1[col] != '')]\n",
    "        corr = np.corrcoef(temp1['target'], temp1[col])[0,1]\n",
    "        corrs_target = corrs_target.append({'col': col, 'corr': corr}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs['abs_corr'] = abs(corrs['corr'])\n",
    "corrs_target['abs_corr'] = abs(corrs_target['corr'])\n",
    "dfi.export(corrs[abs(corrs['corr']) > 0.70].sort_values(by='abs_corr', ascending=False).drop(['abs_corr'], axis=1).style.hide_index().set_table_styles([dict(selector = 'th', props=[('text-align', 'right')])]), 'C:/Users/bbloc/OneDrive/Desktop/NYU_Data_Science/Responsible_DS/Project/figures/func_dependencies.png', max_rows=150)\n",
    "dfi.export(corrs.sort_values(by='abs_corr', ascending=False).drop(['abs_corr'], axis=1).style.hide_index().set_table_styles([dict(selector = 'th', props=[('text-align', 'right')])]), 'C:/Users/bbloc/OneDrive/Desktop/NYU_Data_Science/Responsible_DS/Project/figures/correlations_all_nontarget.png', max_rows=150)\n",
    "dfi.export(corrs[(abs(corrs['corr']) > 0.15) & (abs(corrs['corr']) <= 0.70)].sort_values(by='abs_corr', ascending=False).drop(['abs_corr'], axis=1).reset_index(drop=True).loc[0:20].style.hide_index().set_table_styles([dict(selector = 'th', props=[('text-align', 'right')])]), 'C:/Users/bbloc/OneDrive/Desktop/NYU_Data_Science/Responsible_DS/Project/figures/correlations_nonfunctional_nontarget_1.png', max_rows=150)\n",
    "dfi.export(corrs[(abs(corrs['corr']) > 0.15) & (abs(corrs['corr']) <= 0.70)].sort_values(by='abs_corr', ascending=False).drop(['abs_corr'], axis=1).reset_index(drop=True).loc[21:40].style.hide_index().set_table_styles([dict(selector = 'th', props=[('text-align', 'right')])]), 'C:/Users/bbloc/OneDrive/Desktop/NYU_Data_Science/Responsible_DS/Project/figures/correlations_nonfunctional_nontarget_2.png', max_rows=150)\n",
    "dfi.export(corrs[(abs(corrs['corr']) > 0.15) & (abs(corrs['corr']) <= 0.70)].sort_values(by='abs_corr', ascending=False).drop(['abs_corr'], axis=1).reset_index(drop=True).loc[41:60].style.hide_index().set_table_styles([dict(selector = 'th', props=[('text-align', 'right')])]), 'C:/Users/bbloc/OneDrive/Desktop/NYU_Data_Science/Responsible_DS/Project/figures/correlations_nonfunctional_nontarget_3.png', max_rows=150)\n",
    "dfi.export(corrs[(abs(corrs['corr']) > 0.15) & (abs(corrs['corr']) <= 0.70)].sort_values(by='abs_corr', ascending=False).drop(['abs_corr'], axis=1).reset_index(drop=True).loc[61:80].style.hide_index().set_table_styles([dict(selector = 'th', props=[('text-align', 'right')])]), 'C:/Users/bbloc/OneDrive/Desktop/NYU_Data_Science/Responsible_DS/Project/figures/correlations_nonfunctional_nontarget_4.png', max_rows=150)\n",
    "dfi.export(corrs[(abs(corrs['corr']) > 0.15) & (abs(corrs['corr']) <= 0.70)].sort_values(by='abs_corr', ascending=False).drop(['abs_corr'], axis=1).reset_index(drop=True).loc[81:100].style.hide_index().set_table_styles([dict(selector = 'th', props=[('text-align', 'right')])]), 'C:/Users/bbloc/OneDrive/Desktop/NYU_Data_Science/Responsible_DS/Project/figures/correlations_nonfunctional_nontarget_5.png', max_rows=150)\n",
    "dfi.export(corrs[(abs(corrs['corr']) > 0.15) & (abs(corrs['corr']) <= 0.70)].sort_values(by='abs_corr', ascending=False).drop(['abs_corr'], axis=1).reset_index(drop=True).loc[101:].style.hide_index().set_table_styles([dict(selector = 'th', props=[('text-align', 'right')])]), 'C:/Users/bbloc/OneDrive/Desktop/NYU_Data_Science/Responsible_DS/Project/figures/correlations_nonfunctional_nontarget_6.png', max_rows=150)\n",
    "dfi.export(corrs_target.sort_values(by='abs_corr', ascending=False).drop(['abs_corr'], axis=1).reset_index(drop=True).loc[0:20].style.hide_index().set_table_styles([dict(selector = 'th', props=[('text-align', 'right')])]), 'C:/Users/bbloc/OneDrive/Desktop/NYU_Data_Science/Responsible_DS/Project/figures/correlations_all_target_1.png', max_rows=150)\n",
    "dfi.export(corrs_target.sort_values(by='abs_corr', ascending=False).drop(['abs_corr'], axis=1).reset_index(drop=True).loc[21:40].style.hide_index().set_table_styles([dict(selector = 'th', props=[('text-align', 'right')])]), 'C:/Users/bbloc/OneDrive/Desktop/NYU_Data_Science/Responsible_DS/Project/figures/correlations_all_target_2.png', max_rows=150)\n",
    "dfi.export(corrs_target.sort_values(by='abs_corr', ascending=False).drop(['abs_corr'], axis=1).reset_index(drop=True).loc[41:60].style.hide_index().set_table_styles([dict(selector = 'th', props=[('text-align', 'right')])]), 'C:/Users/bbloc/OneDrive/Desktop/NYU_Data_Science/Responsible_DS/Project/figures/correlations_all_target_3.png', max_rows=150)\n",
    "dfi.export(corrs_target.sort_values(by='abs_corr', ascending=False).drop(['abs_corr'], axis=1).reset_index(drop=True).loc[61:80].style.hide_index().set_table_styles([dict(selector = 'th', props=[('text-align', 'right')])]), 'C:/Users/bbloc/OneDrive/Desktop/NYU_Data_Science/Responsible_DS/Project/figures/correlations_all_target_4.png', max_rows=150)\n",
    "dfi.export(corrs_target.sort_values(by='abs_corr', ascending=False).drop(['abs_corr'], axis=1).reset_index(drop=True).loc[81:100].style.hide_index().set_table_styles([dict(selector = 'th', props=[('text-align', 'right')])]), 'C:/Users/bbloc/OneDrive/Desktop/NYU_Data_Science/Responsible_DS/Project/figures/correlations_all_target_5.png', max_rows=150)\n",
    "dfi.export(corrs_target.sort_values(by='abs_corr', ascending=False).drop(['abs_corr'], axis=1).reset_index(drop=True).loc[101:].style.hide_index().set_table_styles([dict(selector = 'th', props=[('text-align', 'right')])]), 'C:/Users/bbloc/OneDrive/Desktop/NYU_Data_Science/Responsible_DS/Project/figures/correlations_all_target_6.png', max_rows=150)\n",
    "dfi.export(corrs_target[(abs(corrs_target['corr']) >= 0.03)].sort_values(by='abs_corr', ascending=False).drop(['abs_corr'], axis=1).style.hide_index().set_table_styles([dict(selector = 'th', props=[('text-align', 'right')])]), 'C:/Users/bbloc/OneDrive/Desktop/NYU_Data_Science/Responsible_DS/Project/figures/correlations_large_target.png', max_rows=150)\n",
    "display(corrs[abs(corrs['corr']) > 0.70])\n",
    "display(corrs[(abs(corrs['corr']) > 0.15) & (abs(corrs['corr']) <= 0.70)].sort_values(by='abs_corr', ascending=False).drop(['abs_corr'], axis=1))\n",
    "display(corrs_target.sort_values(by='abs_corr', ascending=False).drop(['abs_corr'],axis=1))\n",
    "correlations = train.corr()['target'].sort_values()\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['age_years'] = np.ceil((train['days_birth'] * -1) / 365)\n",
    "train['employed_years'] = np.ceil((train['days_employed'] * -1) / 365)\n",
    "for col in ['code_gender', 'name_education_type', 'name_family_status', 'name_housing_type', 'age_years', 'employed_years']:\n",
    "    print(train['amt_income_total'].median())\n",
    "    display(train.groupby(col)['amt_income_total'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From this point on, we will implement Hanane's final model and examine the outcomes\n",
    "\n",
    "df = train_in.copy()\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = df[col].str.lower()\n",
    "\n",
    "label_encode = LabelEncoder()\n",
    "\n",
    "for col in df:\n",
    "    if df[col].dtype == 'object':\n",
    "        # If 2 or fewer unique categories\n",
    "        if len(list(df[col].unique())) <= 2:\n",
    "            label_encode.fit(df[col])\n",
    "            df[col] = label_encode.transform(df[col])\n",
    "            \n",
    "\n",
    "df = pd.get_dummies(df)\n",
    "display(df.head(1))\n",
    "\n",
    "ft_list = ['EXT_SOURCE_3',\n",
    " 'EXT_SOURCE_2',\n",
    " 'EXT_SOURCE_1',\n",
    " 'NAME_EDUCATION_TYPE_Higher education',\n",
    " 'CODE_GENDER_F',\n",
    " 'NAME_INCOME_TYPE_Pensioner',\n",
    " 'ORGANIZATION_TYPE_XNA',\n",
    " 'DAYS_EMPLOYED',\n",
    " 'FLOORSMAX_AVG',\n",
    " 'FLOORSMAX_MEDI',\n",
    " 'FLOORSMAX_MODE',\n",
    " 'EMERGENCYSTATE_MODE_NO',\n",
    " 'HOUSETYPE_MODE_block of flats',\n",
    " 'AMT_GOODS_PRICE',\n",
    " 'REGION_POPULATION_RELATIVE',\n",
    " 'OWN_CAR_AGE',\n",
    " 'DAYS_REGISTRATION',\n",
    " 'OCCUPATION_TYPE_Laborers',\n",
    " 'FLAG_DOCUMENT_3',\n",
    " 'REG_CITY_NOT_LIVE_CITY',\n",
    " 'FLAG_EMP_PHONE',\n",
    " 'NAME_EDUCATION_TYPE_Secondary / secondary special',\n",
    " 'REG_CITY_NOT_WORK_CITY',\n",
    " 'DAYS_ID_PUBLISH',\n",
    " 'CODE_GENDER_M',\n",
    " 'DAYS_LAST_PHONE_CHANGE',\n",
    " 'NAME_INCOME_TYPE_Working',\n",
    " 'REGION_RATING_CLIENT',\n",
    " 'REGION_RATING_CLIENT_W_CITY',\n",
    " 'DAYS_BIRTH',\n",
    " 'TARGET']\n",
    "\n",
    "ft_list = [x.lower() for x in ft_list]\n",
    "df = df[ft_list]\n",
    "\n",
    "# create X_train, y_train\n",
    "X_train = df.drop('target', axis = 1)\n",
    "y_train = df['target']\n",
    "target = y_train\n",
    "features = X_train\n",
    "# Feature names\n",
    "features_list = list(X_train.columns)\n",
    "\n",
    "#Impute missing values using median strategy\n",
    "imputer = SimpleImputer(strategy = 'median')\n",
    "\n",
    "#Scale all values in 0-1 range\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "#fit the imputer on training set\n",
    "imputer.fit(X_train)\n",
    "\n",
    "#transform both training and testing data\n",
    "X_train = imputer.transform(X_train)\n",
    "\n",
    "#fit scaler and transform\n",
    "scaler.fit(X_train)\n",
    "train_scaled = scaler.transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_lgbm_model = lgbm.LGBMClassifier(n_estimators= 500, learning_rate = 0.09913706634323802, \n",
    "                                       num_leaves = 900, max_depth = 8, min_data_in_leaf = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)\n",
    "tuned_lgbm_model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_lgbm_predictions = tuned_lgbm_model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores (preds): \n",
    "    print('Accuracy: %.3f' % accuracy_score(test_labels.values, preds))\n",
    "    print('Precision: %.3f' % precision_score(test_labels.values, preds))\n",
    "    print('Recall: %.3f' % recall_score(test_labels.values, preds))\n",
    "    print('F1: %.3f' % f1_score(test_labels.values, preds))\n",
    "\n",
    "print('Tuned LGBM scores: ')\n",
    "get_scores(tuned_lgbm_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tuned_lgbm_predictions))\n",
    "print(np.sum(tuned_lgbm_predictions))\n",
    "print(np.sum(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(test_features, columns=[x for x in df.columns if x != 'target'])\n",
    "df_test['true'] = list(test_labels)\n",
    "df_test['pred'] = tuned_lgbm_predictions\n",
    "\n",
    "df_test['age_years'] = np.ceil((df_test['days_birth'] * -1) / 365)\n",
    "df_test['emp_years'] = np.ceil((df_test['days_employed'] * -1) / 365)\n",
    "\n",
    "df_test['age_group'] = ''\n",
    "df_test['age_group'] = np.where((df_test['age_years'] < 30), '<30', df_test['age_group'])\n",
    "df_test['age_group'] = np.where((df_test['age_years'] >= 30) & (df_test['age_years'] < 40), '30-39', df_test['age_group'])\n",
    "df_test['age_group'] = np.where((df_test['age_years'] >= 40) & (df_test['age_years'] < 50), '40-49', df_test['age_group'])\n",
    "df_test['age_group'] = np.where((df_test['age_years'] >= 50) & (df_test['age_years'] < 60), '50-59', df_test['age_group'])\n",
    "df_test['age_group'] = np.where((df_test['age_years'] >= 60), '60+', df_test['age_group'])\n",
    "\n",
    "df_test['emp_group'] = ''\n",
    "df_test['emp_group'] = np.where((df_test['emp_years'] >= 0) & (df_test['emp_years'] < 5), '0-4', df_test['emp_group'])\n",
    "df_test['emp_group'] = np.where((df_test['emp_years'] >= 5) & (df_test['emp_years'] < 10), '5-9', df_test['emp_group'])\n",
    "df_test['emp_group'] = np.where((df_test['emp_years'] >= 10) & (df_test['emp_years'] < 20), '10-19', df_test['emp_group'])\n",
    "df_test['emp_group'] = np.where((df_test['emp_years'] >= 20), '20+', df_test['emp_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(df, var):\n",
    "\n",
    "    table = pd.DataFrame()\n",
    "    \n",
    "    for sub_pop in df[var].unique():\n",
    "\n",
    "        df_filt = df.copy()\n",
    "        df_filt = df_filt[df_filt[var] == sub_pop]\n",
    "        fp = 0\n",
    "        tp = 0\n",
    "        fn = 0\n",
    "        tn = 0\n",
    "        for index, row in df_filt.iterrows():\n",
    "            if row['pred'] == 1 and row['true'] == 0:\n",
    "                fp += 1\n",
    "            elif row['pred'] == 0 and row['true'] == 1:\n",
    "                fn += 1\n",
    "            elif row['pred'] == 1 and row['true'] == 1:\n",
    "                tp += 1\n",
    "            elif row['pred'] == 0 and row['true'] == 0:\n",
    "                tn += 1\n",
    "                \n",
    "        accuracy = (tp + tn) / len(df_filt)\n",
    "        fp_rate = fp / (fp + tn)\n",
    "        fn_rate = 1 - (fn / (tp + fn))\n",
    "        \n",
    "        table = table.append({'group': var + ' ' + str(sub_pop), 'acc': accuracy, 'fp': fp_rate, 'fn': fn_rate}, ignore_index=True)\n",
    "\n",
    "#         print(\"Accuracy for {} {}: {:.1%}\".format(var, sub_pop, accuracy))\n",
    "#         print(\"FP Rate for {} {}: {:.1%}\".format(var, sub_pop, fp_rate))\n",
    "#         print(\"FN Rate for {} {}: {:.1%}\".format(var, sub_pop, fn_rate))\n",
    "#         print(\"\\n\")\n",
    "    display(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(df_test, 'code_gender_f')\n",
    "compute_metrics(df_test, 'age_group')\n",
    "compute_metrics(df_test, 'emp_group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = train.copy()\n",
    "temp = temp[(temp['floorsmax_avg'].isnull() == False) & (temp['floorsmax_avg'] != '') & (temp['floorsmax_medi'].isnull() == False) & (temp['floorsmax_medi'] != '')]\n",
    "corr = np.corrcoef(temp['floorsmax_avg'], temp['floorsmax_medi'])[0,1]\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import BinaryLabelDataset, StandardDataset\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "from aif360.algorithms.inprocessing import PrejudiceRemover\n",
    "from aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing, RejectOptionClassification\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.metrics import BinaryLabelDatasetMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert this dataframe into an aif360 dataset\n",
    "df_nonull = df.copy()\n",
    "for col in df_nonull.columns:\n",
    "    df_nonull = df_nonull[(df_nonull[col].isnull() == False) & (df_nonull[col] != '')]\n",
    "protected_attr = 'code_gender_f'\n",
    "target = 'target'\n",
    "dataset_orig = BinaryLabelDataset(\n",
    "    favorable_label=0,\n",
    "    unfavorable_label=1,\n",
    "    df=df_nonull,\n",
    "    label_names=[target],\n",
    "    protected_attribute_names=[protected_attr])\n",
    "privileged_groups = [{protected_attr: 0}] \n",
    "unprivileged_groups = [{protected_attr: 1}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dataframes\n",
    "train_orig, test_orig = dataset_orig.split([0.8], shuffle=True, seed=10)\n",
    "train_orig, val_orig = train_orig.split([0.875], shuffle=True, seed=5)\n",
    "\n",
    "# Convert to dataframes\n",
    "train_orig_df, _ = train_orig.convert_to_dataframe()\n",
    "val_orig_df, _ = val_orig.convert_to_dataframe()\n",
    "test_orig_df, _ = test_orig.convert_to_dataframe()\n",
    "\n",
    "print(\"Train set: \", train_orig_df.shape)\n",
    "print(\"Val set: \", val_orig_df.shape)\n",
    "print(\"Test set: \", test_orig_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_aif = lgbm.LGBMClassifier(n_estimators= 500, learning_rate = 0.09913706634323802, \n",
    "                                       num_leaves = 900, max_depth = 8, min_data_in_leaf = 600)\n",
    "\n",
    "X_train_aif = train_orig_df.drop(['target'],axis=1).values\n",
    "y_train_aif = train_orig_df[['target']].values\n",
    "X_test_aif = test_orig_df.drop(['target'],axis=1).values\n",
    "y_test_aif = test_orig_df[['target']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_aif.fit(X_train_aif, y_train_aif)\n",
    "y_pred = lgbm_aif.predict(X_test_aif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(y_pred, y_test):\n",
    "\n",
    "    correct = 0\n",
    "\n",
    "    for pred, true in zip(y_pred, y_test):\n",
    "        if pred == true:\n",
    "            correct += 1\n",
    "\n",
    "    return correct / len(y_pred)\n",
    "\n",
    "def compute_acc_group(y_pred, y_test, test_orig_df):\n",
    "\n",
    "    test_orig_df = test_orig_df.reset_index(drop=True)\n",
    "\n",
    "    correct_priv = 0\n",
    "    correct_unpriv = 0\n",
    "\n",
    "    total_priv = len(test_orig_df[test_orig_df['code_gender_f'] == 0])\n",
    "    total_unpriv = len(test_orig_df[test_orig_df['code_gender_f'] == 1])\n",
    "\n",
    "    for index, row in test_orig_df.iterrows():\n",
    "        if row['code_gender_f'] == 0 and y_pred[index] == y_test[index][0]:\n",
    "            correct_priv += 1\n",
    "        elif row['code_gender_f'] == 1 and y_pred[index] == y_test[index][0]:\n",
    "            correct_unpriv += 1\n",
    "    \n",
    "    return correct_priv / total_priv, correct_unpriv / total_unpriv\n",
    "\n",
    "def compute_fp(y_pred, y_test, test_orig_df):\n",
    "\n",
    "    fp_priv = 0\n",
    "    tn_priv = 0\n",
    "    fp_unpriv = 0\n",
    "    tn_unpriv = 0\n",
    "\n",
    "    test_orig_df = test_orig_df.reset_index(drop=True)\n",
    "\n",
    "    for index, row in test_orig_df.iterrows():\n",
    "        if row['code_gender_f'] == 0 and y_pred[index] == 1 and y_test[index][0] == 0:\n",
    "            fp_priv += 1\n",
    "        elif row['code_gender_f'] == 0 and y_pred[index] == 0 and y_test[index][0] == 0:\n",
    "            tn_priv += 1\n",
    "        elif row['code_gender_f'] == 1 and y_pred[index] == 1 and y_test[index][0] == 0:\n",
    "            fp_unpriv += 1\n",
    "        elif row['code_gender_f'] == 1 and y_pred[index] == 0 and y_test[index][0] == 0:\n",
    "            tn_unpriv += 1\n",
    "\n",
    "    fp_rate_priv = fp_priv / (fp_priv + tn_priv)\n",
    "    fp_rate_unpriv = fp_unpriv / (fp_unpriv + tn_unpriv)\n",
    "\n",
    "    return fp_rate_priv, fp_rate_unpriv\n",
    "\n",
    "def calc_disparate_impact(df, y_pred):\n",
    "\n",
    "    df_pred = df.copy()\n",
    "    df_pred['target'] = y_pred\n",
    "\n",
    "    privileged_groups = [{'code_gender_f': 0}]\n",
    "    unprivileged_groups = [{'code_gender_f': 1}]  \n",
    "\n",
    "    aif360_test = StandardDataset(df_pred, label_name='target', protected_attribute_names=['code_gender_f'], \n",
    "                privileged_classes=[[1]], favorable_classes=[1])\n",
    "  \n",
    "    metric_preds = BinaryLabelDatasetMetric(\n",
    "    aif360_test, \n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    "  )\n",
    "\n",
    "    disparate_impact = metric_preds.disparate_impact()\n",
    "\n",
    "    return disparate_impact\n",
    "\n",
    "def calc_fp_diff(df, y_pred):\n",
    "\n",
    "    privileged_groups = [{'code_gender_f': 0}]\n",
    "    unprivileged_groups = [{'code_gender_f': 1}]\n",
    "\n",
    "    orig_aif360 = StandardDataset(df, label_name='target', protected_attribute_names=['code_gender_f'], \n",
    "                privileged_classes=[[1]], favorable_classes=[1])\n",
    "    pred_test_df = df.copy()\n",
    "    pred_test_df['target'] = y_pred\n",
    "    preds_aif360 = StandardDataset(pred_test_df, label_name='target', protected_attribute_names=['code_gender_f'], \n",
    "                  privileged_classes=[[1]], favorable_classes=[1])\n",
    "\n",
    "    orig_vs_preds_metrics = ClassificationMetric(orig_aif360, preds_aif360,\n",
    "                                                    unprivileged_groups=unprivileged_groups,\n",
    "                                                    privileged_groups=privileged_groups)\n",
    "\n",
    "    fp_rate_priv = orig_vs_preds_metrics.false_positive_rate(privileged=True)\n",
    "    fp_rate_unpriv = orig_vs_preds_metrics.false_positive_rate(privileged=False)\n",
    "\n",
    "    fp_diff = fp_rate_unpriv - fp_rate_priv\n",
    "\n",
    "    return fp_diff\n",
    "\n",
    "accuracy = compute_acc(y_pred, y_test_aif)\n",
    "accuracy_priv, accuracy_unpriv = compute_acc_group(y_pred, y_test_aif, test_orig_df)\n",
    "disparate_impact = calc_disparate_impact(test_orig_df, y_pred)\n",
    "fp_diff = calc_fp_diff(test_orig_df, y_pred)\n",
    "\n",
    "print(\"Accuracy of the model: {:.3%}\".format(accuracy))\n",
    "print(\"Accuracy of the privelaged group: {:.3%}\".format(accuracy_priv))\n",
    "print(\"Accuracy of the unprivelaged group: {:.3%}\".format(accuracy_unpriv))\n",
    "print(\"Disparate Impact: {:.3}\".format(disparate_impact))\n",
    "print(\"False Positive Rate Difference: {:.3%}\".format(fp_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train['code_gender'] == 'M']['target'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of Loan Default for Males')\n",
    "plt.xlabel(\"Credit (1 = Default, 0 = Default)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "train[train['code_gender'] == 'F']['target'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of Loan Default for Females')\n",
    "plt.xlabel(\"(1 = Default, 0 = No Default)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "print(len(train[(train['code_gender'] == 'M') & (train['target'] == 1)]) / len(train[(train['code_gender'] == 'M')]))\n",
    "print(len(train[(train['code_gender'] == 'F') & (train['target'] == 1)]) / len(train[(train['code_gender'] == 'F')]))\n",
    "\n",
    "print(len(train[(train['code_gender'] == 'M') & (train['amt_income_total'].isnull() == True)]) / len(train[(train['code_gender'] == 'M')]))\n",
    "print(len(train[(train['code_gender'] == 'F') & (train['amt_income_total'].isnull() == True)]) / len(train[(train['code_gender'] == 'F')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert this dataframe into an aif360 dataset\n",
    "df_nonull = df.copy()\n",
    "df_nonull['age_years'] = np.ceil((df_nonull['days_birth'] * -1) / 365)\n",
    "df_nonull['age_group'] = np.where(df_nonull['age_years'] < 40, 1, 0)\n",
    "for col in df_nonull.columns:\n",
    "    df_nonull = df_nonull[(df_nonull[col].isnull() == False) & (df_nonull[col] != '')]\n",
    "protected_attr = 'age_group'\n",
    "target = 'target'\n",
    "dataset_orig = BinaryLabelDataset(\n",
    "    favorable_label=0,\n",
    "    unfavorable_label=1,\n",
    "    df=df_nonull,\n",
    "    label_names=[target],\n",
    "    protected_attribute_names=[protected_attr])\n",
    "privileged_groups = [{protected_attr: 0}] \n",
    "unprivileged_groups = [{protected_attr: 1}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dataframes\n",
    "train_orig, test_orig = dataset_orig.split([0.8], shuffle=True, seed=10)\n",
    "train_orig, val_orig = train_orig.split([0.875], shuffle=True, seed=5)\n",
    "\n",
    "# Convert to dataframes\n",
    "train_orig_df, _ = train_orig.convert_to_dataframe()\n",
    "val_orig_df, _ = val_orig.convert_to_dataframe()\n",
    "test_orig_df, _ = test_orig.convert_to_dataframe()\n",
    "\n",
    "print(\"Train set: \", train_orig_df.shape)\n",
    "print(\"Val set: \", val_orig_df.shape)\n",
    "print(\"Test set: \", test_orig_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_aif = lgbm.LGBMClassifier(n_estimators= 500, learning_rate = 0.09913706634323802, \n",
    "                                       num_leaves = 900, max_depth = 8, min_data_in_leaf = 600)\n",
    "\n",
    "X_train_aif = train_orig_df.drop(['target'],axis=1).values\n",
    "y_train_aif = train_orig_df[['target']].values\n",
    "X_test_aif = test_orig_df.drop(['target'],axis=1).values\n",
    "y_test_aif = test_orig_df[['target']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_aif.fit(X_train_aif, y_train_aif)\n",
    "y_pred = lgbm_aif.predict(X_test_aif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(y_pred, y_test):\n",
    "\n",
    "    correct = 0\n",
    "\n",
    "    for pred, true in zip(y_pred, y_test):\n",
    "        if pred == true:\n",
    "            correct += 1\n",
    "\n",
    "    return correct / len(y_pred)\n",
    "\n",
    "def compute_acc_group(y_pred, y_test, test_orig_df):\n",
    "\n",
    "    test_orig_df = test_orig_df.reset_index(drop=True)\n",
    "\n",
    "    correct_priv = 0\n",
    "    correct_unpriv = 0\n",
    "\n",
    "    total_priv = len(test_orig_df[test_orig_df['age_group'] == 0])\n",
    "    total_unpriv = len(test_orig_df[test_orig_df['age_group'] == 1])\n",
    "\n",
    "    for index, row in test_orig_df.iterrows():\n",
    "        if row['age_group'] == 0 and y_pred[index] == y_test[index][0]:\n",
    "            correct_priv += 1\n",
    "        elif row['age_group'] == 1 and y_pred[index] == y_test[index][0]:\n",
    "            correct_unpriv += 1\n",
    "    \n",
    "    return correct_priv / total_priv, correct_unpriv / total_unpriv\n",
    "\n",
    "def compute_fp(y_pred, y_test, test_orig_df):\n",
    "\n",
    "    fp_priv = 0\n",
    "    tn_priv = 0\n",
    "    fp_unpriv = 0\n",
    "    tn_unpriv = 0\n",
    "\n",
    "    test_orig_df = test_orig_df.reset_index(drop=True)\n",
    "\n",
    "    for index, row in test_orig_df.iterrows():\n",
    "        if row['age_group'] == 0 and y_pred[index] == 1 and y_test[index][0] == 0:\n",
    "            fp_priv += 1\n",
    "        elif row['age_group'] == 0 and y_pred[index] == 0 and y_test[index][0] == 0:\n",
    "            tn_priv += 1\n",
    "        elif row['age_group'] == 1 and y_pred[index] == 1 and y_test[index][0] == 0:\n",
    "            fp_unpriv += 1\n",
    "        elif row['age_group'] == 1 and y_pred[index] == 0 and y_test[index][0] == 0:\n",
    "            tn_unpriv += 1\n",
    "\n",
    "    fp_rate_priv = fp_priv / (fp_priv + tn_priv)\n",
    "    fp_rate_unpriv = fp_unpriv / (fp_unpriv + tn_unpriv)\n",
    "\n",
    "    return fp_rate_priv, fp_rate_unpriv\n",
    "\n",
    "def calc_disparate_impact(df, y_pred):\n",
    "\n",
    "    df_pred = df.copy()\n",
    "    df_pred['target'] = y_pred\n",
    "\n",
    "    privileged_groups = [{'age_group': 0}]\n",
    "    unprivileged_groups = [{'age_group': 1}]  \n",
    "\n",
    "    aif360_test = StandardDataset(df_pred, label_name='target', protected_attribute_names=['age_group'], \n",
    "                privileged_classes=[[1]], favorable_classes=[1])\n",
    "  \n",
    "    metric_preds = BinaryLabelDatasetMetric(\n",
    "    aif360_test, \n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    "  )\n",
    "\n",
    "    disparate_impact = metric_preds.disparate_impact()\n",
    "\n",
    "    return disparate_impact\n",
    "\n",
    "def calc_fp_diff(df, y_pred):\n",
    "\n",
    "    privileged_groups = [{'age_group': 0}]\n",
    "    unprivileged_groups = [{'age_group': 1}]\n",
    "\n",
    "    orig_aif360 = StandardDataset(df, label_name='target', protected_attribute_names=['age_group'], \n",
    "                privileged_classes=[[1]], favorable_classes=[1])\n",
    "    pred_test_df = df.copy()\n",
    "    pred_test_df['target'] = y_pred\n",
    "    preds_aif360 = StandardDataset(pred_test_df, label_name='target', protected_attribute_names=['age_group'], \n",
    "                  privileged_classes=[[1]], favorable_classes=[1])\n",
    "\n",
    "    orig_vs_preds_metrics = ClassificationMetric(orig_aif360, preds_aif360,\n",
    "                                                    unprivileged_groups=unprivileged_groups,\n",
    "                                                    privileged_groups=privileged_groups)\n",
    "\n",
    "    fp_rate_priv = orig_vs_preds_metrics.false_positive_rate(privileged=True)\n",
    "    fp_rate_unpriv = orig_vs_preds_metrics.false_positive_rate(privileged=False)\n",
    "\n",
    "    fp_diff = fp_rate_unpriv - fp_rate_priv\n",
    "\n",
    "    return fp_diff\n",
    "\n",
    "accuracy = compute_acc(y_pred, y_test_aif)\n",
    "accuracy_priv, accuracy_unpriv = compute_acc_group(y_pred, y_test_aif, test_orig_df)\n",
    "disparate_impact = calc_disparate_impact(test_orig_df, y_pred)\n",
    "fp_diff = calc_fp_diff(test_orig_df, y_pred)\n",
    "\n",
    "print(\"Accuracy of the model: {:.3%}\".format(accuracy))\n",
    "print(\"Accuracy of the privelaged group: {:.3%}\".format(accuracy_priv))\n",
    "print(\"Accuracy of the unprivelaged group: {:.3%}\".format(accuracy_unpriv))\n",
    "print(\"Disparate Impact: {:.3}\".format(disparate_impact))\n",
    "print(\"False Positive Rate Difference: {:.3%}\".format(fp_diff))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
